{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EECS 504 PS4: Backpropagation",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n",
        "Please provide the following information \n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Nidhi Sridhar, nidhisri\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.     \n",
        "    y = np.dot(x, w) + b\n",
        "    #print(x.shape)\n",
        "    \n",
        "             #\n",
        "    ###########################################################################\n",
        "    out = y\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    (x, w, b) = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    #x = cache[0]\n",
        "    #w = cache[1]\n",
        "    #b = cache[2]\n",
        "    #print(x.shape)\n",
        "    #w = w.reshape(-1,1)\n",
        "    #dout = dout.reshape(-1,1)\n",
        "    #x = x.reshape(-1,1)\n",
        "    dx = np.dot(dout, w.T)\n",
        "\n",
        "    dw = np.dot(x.T, dout)\n",
        "    db = np.sum(dout, axis = 0)\n",
        "    #db = np.multiply(dout, 1)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out = np.maximum(0,x)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    x = cache\n",
        "    #dx = dx.reshape(-1,1)\n",
        "    A = np.maximum(0,x)\n",
        "    A[A>0] = 1\n",
        "    dx = np.multiply(A, dx)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                           #\n",
        "    ###########################################################################\n",
        "    x_max = np.max(x, axis = 1, keepdims = True)\n",
        "    p = np.exp(x - x_max)\n",
        "    p = p/np.sum(p, axis = 1, keepdims = True)\n",
        "    loss = 0\n",
        "    #dx = np.zeros(p.shape[0])\n",
        "    N = x.shape[0]\n",
        "    '''for i in range(N):\n",
        "      loss =- y[i]*np.log(p[i])\n",
        "      dx[i] = y[i] - p[i]'''\n",
        "    loss = -np.sum(np.log(p[range(N), y])) / N\n",
        "    #m = y.shape[0]\n",
        "    grad = p.copy()\n",
        "    grad[range(N),y] -= 1\n",
        "    grad = grad/N\n",
        "    dx = grad\n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        #self.hidden_dim = \n",
        "        self.params['W1'] = np.random.normal(scale = weight_scale, size = (input_dim, hidden_dim))\n",
        "        self.params['W2'] = np.random.normal(scale = weight_scale, size = (hidden_dim, num_classes))\n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        out1, cache1 = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        out2, cache2 = relu_forward(out1)\n",
        "        scores, cache3 = fc_forward(out2, self.params['W2'], self.params['b2'])\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, dx1 = softmax_loss(scores, y)\n",
        "        #dx2, grads['W2'], grads['b2'] = fc_backward(dx1, cache1)\n",
        "        dx2, dW2, db2 = fc_backward(dx1, cache3)\n",
        "        dx3 = relu_backward(dx2, cache2)\n",
        "        dx4, dW1, db1 = fc_backward(dx3, cache1)\n",
        "        grads = {\"W1\": dW1, \"W2\": dW2, \"b1\": db1, \"b2\":db2}\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "outputId": "ac95513b-d616-4bae-97b2-6204e24eebbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters                   \n",
        "'''self.X_train = data['X_train']\n",
        "self.y_train = data['y_train']\n",
        "self.x_val = data['x_val']\n",
        "self.y_val = data['y_val'] '''         #\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 3e-2,\n",
        "    lr_decay= 1.05, num_epochs=10, \n",
        "    batch_size= 200, print_every=1000)\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 2000) loss: 2.313376\n",
            "(Epoch 0 / 10) train acc: 0.121000; val_acc: 0.123800\n",
            "(Epoch 1 / 10) train acc: 0.440000; val_acc: 0.384200\n",
            "(Epoch 2 / 10) train acc: 0.455000; val_acc: 0.424500\n",
            "(Epoch 3 / 10) train acc: 0.496000; val_acc: 0.454600\n",
            "(Epoch 4 / 10) train acc: 0.504000; val_acc: 0.471400\n",
            "(Epoch 5 / 10) train acc: 0.546000; val_acc: 0.487400\n",
            "(Iteration 1001 / 2000) loss: 1.451665\n",
            "(Epoch 6 / 10) train acc: 0.548000; val_acc: 0.494200\n",
            "(Epoch 7 / 10) train acc: 0.586000; val_acc: 0.504000\n",
            "(Epoch 8 / 10) train acc: 0.608000; val_acc: 0.507100\n",
            "(Epoch 9 / 10) train acc: 0.619000; val_acc: 0.508600\n",
            "(Epoch 10 / 10) train acc: 0.647000; val_acc: 0.513600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "outputId": "6800de6a-57ab-477f-d989-2d37a8be9898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab_type": "code",
        "outputId": "aff8cd0e-c5e2-4fc6-8e85-a8d8944b3f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "'''history = model.fit(X_train, y_train, validation_data=(x_val, y_val), batch_size=30, epochs=10, verbose=1)\n",
        "train_acc_history = history.history('train_acc_history')\n",
        "val_acc_history = history.history('val_acc_history')'''\n",
        "plt.plot(train_acc_history, 'ro--')\n",
        "plt.plot(val_acc_history , 'bo-')\n",
        "plt.title('Train and Val Accuracy vs Epochs')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show();"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCi_oaWY8hgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}